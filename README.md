# Reservation Agents


### Recent updates ðŸ“£
* *June 2025 (v0.0.2)*: Create chat demo `README.md`.
* *June 2025 (v0.0.1)*: Create chat demo codes using FastAPI communication.

&nbsp;

&nbsp;



## Overview ðŸ“š
<!-- This repository is designed to make it easy for anyone to tune models available on Hugging Face.
When a new model is released, anyone can easily implement a model wrapper to perform instruction-tuning and fine-tuning.
For detailed usage instructions, please refer to the description below.
* Universal LLM trainer supports full-training.
* Universal LLM trainer supports LoRA fine-tuning.
* Universal LLM trainer supports QLoRA fine-tuning.
* Universal LLM trainer supports DDP and FSDP training strategies.

&nbsp; -->



&nbsp;

&nbsp;

<!-- ## Quick Starts ðŸš€
### Environment setup
We have to install PyTorch and other requirements. Please refer to more [detailed setup](./docs/1_getting_started.md) including Docker.
```bash
# PyTorch Install
pip3 install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124

# Requirements Install
pip3 install -r docker/requirements.txt
```

&nbsp;

### Data preparation
```bash
python3 src/run/dataset_download.py --dataset allenai/ai2_arc --download_path data_examples
```

&nbsp;

### LLM training
```bash
# Llama 3.1 8B LoRA fine-tuning
python3 src/run/train.py --config config/example_llama3.1_lora.yaml --mode train

# Llama 3.1 8B QLoRA fine-tuning
python3 src/run/train.py --config config/example_llama3.1_qlora.yaml --mode train

# Llama 3.1 8B full fine-tuning
python3 src/run/train.py --config config/example_llama3.1_full.yaml --mode train
```

&nbsp;

&nbsp; -->


## Tutorials & Documentations
1. [Getting Started](./docs/1_getting_started.md)
<!-- 2. [Data Preparation](./docs/2_data_preparation.md)
3. [Training](./docs/3_training.md) -->

&nbsp;

&nbsp;

<!-- ## Bug Reports
If an error occurs while executing the code, check if any of the cases below apply.
* [Bug Cases](./docs/bugs.md) -->
